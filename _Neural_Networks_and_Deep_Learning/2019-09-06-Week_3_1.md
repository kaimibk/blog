---
layout: post
title:  "Week 3: Shallow Neural Networks"
categories: Neural_Networks_and_Deep_Learning, notes
date:   2019-09-06 09:00:00 -1000
mathjax: true
---

## Video 1: Neural Networks Overview

Recall, for logistic regression we input the features, $$x$$, and the parameters $$w$$ and $$b$$ to compute $$z = w^T x + b$$. We then used this result to compute $$a \equiv \hat{y} = \sigma(z)$$. Using this, we compute the loss function, $$\mathcal{L}(a,y)$$.

Building upon this idea, we can form neural networks by stacking together a lot of little sigmoid units. However, each "layer" will have its own $$z$$ and $$a$$ computation. Let us define a superscript $$[n]$$ to refer to n-th layer computations. The first layer will in our hypothetical neural network will compute,

$$z^{[1]} = W^{[1]} + b^{b[1]}, \quad a^{[2]} = \sigma\left(z^{[1]}\right)$$ 

Our second layer will perform the computation of, 

$$z^{[2]} = W^{[2]} a^{[1]} + b^{[2]}, \quad a^{[2]} = \sigma\left(z^{[2]}\right)$$

Then, we will compute the loss, $$\mathcal{L}\left(a^{[2]}, y\right)$$.

Similarly to logisitic regression, we are able to perform "backward" computations to compute derivatives, i.e., $$da^{[2]}$$, $$dz^{[2]}$$, etc.

## Video 2: Neural Network Representation

The structure of a neural network is comprised of (1) an input layer, (2) one or more hidden layers, and (3) an output layer which generates the predicated value, $$\hat{y}$$. The reason why the middle element is considered "hidden" is because the true values of those nodes are not observed in the training set.

The input layer passes on the value of X to the hidden layer, so we refer to this as activation, $$a^{[0]}$$. Next, the hidden layer will generate a set of activations $$a^{[1]} = \left\{ a^{[1]}_1,  a^{[1]}_2, \dots ,  a^{[1]}_m \right\}$$. Finally, the output layer will generate a real number, $$a^{[2]} \equiv \hat{y}$$. 

## Video 3: Computing a Neural Network's Output

## Video 4: Vectorizing Across Multiple Examples

## Video 5: Explanation for Vectorized Implementation

## Reading 1: Clarification about Activation Function

## Video 7: Activation Functions

## Video 8: Why Do You Need Non-Linear Activation Functions?

## Video 9: Derivates of Activation Functions

## Video 10: Gradient Descent for Neural Networks

## Reading 2: Clarification about Upcoming Backpropagation Intuition

## Video 11: Backpropagation Intuition

## Video 12: Randon Initialization
