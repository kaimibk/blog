---
layout: post
title:  "Chapter 1: Role of Probability Theory in Science"
categories: notes
date:   2019-05-16 12:00:00 -1000
---

### (1.1) Scientific Inference

Our state of knowledge is always incomplete:
- Measurements are limited by accuracy
- More experiments could be conducted

The goal of inference is to find the truth with incomplete information

### (1.2) Inference Requires Probability Theory

In conventional statistics, the probability is defined as as the frequency of an event&mdash;the "frequentist" perspective.

In the frequentist view, probabilities are restricted to random variables, quantities that can meaningfully vary through a series of repeated experiments.

- $$P(A)$$ : frequency with which A occurs in identical repeats of an experiment

Recently, a new perspective finds that probability theory goes beyond finding the frequency of random variables. Rather, it is a logical framework for conducting inference about hypothesis&mdash;Bayesian probability theory.

This allows us to compute the probability of a theory or model parameter, which frequentist statistics fails to do directly.

- $$P(A \mid B)$$ : a real number measure of the plausibility of a proposition A, given (conditional on) the truth of the information represented by proposition B.


### (1.2.1) : The Two Rules for Manipulating Probabilities

**Sum Rule**
$$P(A \mid B) + P(\sim A \mid B) = 1$$

**Product Rule**

$$P(A,B \mid C)=P(A \mid C)P(B \mid A, C)=P(B \mid C)P(A \mid B,C)$$

where the term $$P(A \mid B)$$ can be interpreted as the probability of the truth of proposition $$A$$, given (conditional on) the truth of the information represented by proposition $$B$$. Bayes Theorem then follows from the product rule by rearranging the middle and right equations,

$$P(A \mid B,C)= \frac{P(A \mid C) P(B \mid A,C)}{P(B \mid C)}$$

### (1.3) : Usual Form of Bayes' Theorem

$$P(H_i|D, I)=(P(H_i│I)P(D|H_i,I))/(P(D|I))$$
- $$H_i$$ : proposition asserting the truth of a hypothesis.
- $$I$$ : proposition representing our prior information.
- $$D$$ : proposition representing data.
- $$P(D│H_i,I)$$ : probability of obtaining the data, $$D$$, if $$H_i$$  and $$I$$ are true. Also called the likelihood.
- $$P(H_i│I)$$ : prior probability of the hypothesis.
- $$P(H_i│D,I)$$ : posterior probability of $$H_i$$.
- $$P(D│I)= \sum_i  P(H_i│I)p(D│H_i,I)$$ : normalization factor.
