---
layout: post
title:  "Bayesian Logical Data Analysis&mdash;Chapter 1"
categories: notes
date:   2019-05-16 12:00:00 -1000
---

Notes on the book, _[Bayesian Logical Data Analysis for the Physical Sciences](https://www.amazon.com/Bayesian-Logical-Analysis-Physical-Sciences/dp/0521150124/ref=sr_1_fkmrnull_3?crid=1B8EBBP98CCSR&keywords=bayesian+logical+data+analysis+for+the+physical+sciences&qid=1558065973&s=gateway&sprefix=Bayesian+logical%2Caps%2C222&sr=8-3-fkmrnull)_ by Phil Gregory. These notes are not meant to be complete, just key information and examples from the text.

## Chapter 1: Role of Probability Theory in Science

### (1.1) Scientific Inference

Our state of knowledge is always incomplete:
- Measurements are limited by accuracy
- More experiments could be conducted

The goal of inference is to find the truth with incomplete information

### (1.2) Inference Requires Probability Theory

In conventional statistics, the probability is defined as as the frequency of an event&mdash;the "frequentist" perspective.

In the frequentist view, probabilities are restricted to random variables, quantities that can meaningfully vary through a series of repeated experiments.

- $$P(A)$$ : frequency with which A occurs in identical repeats of an experiment

Recently, a new perspective finds that probability theory goes beyond finding the frequency of random variables. Rather, it is a logical framework for conducting inference about hypothesis&mdash;Bayesian probability theory.

This allows us to compute the probability of a theory or model parameter, which frequentist statistics fails to do directly.

- $$P(A \mid B)$$ : a real number measure of the plausibility of a proposition A, given (conditional on) the truth of the information represented by proposition B.


### (1.2.1) : The Two Rules for Manipulating Probabilities

**Sum Rule**
$$P(A \mid B) + P(~A \mid B) = 1$$

**Product Rule**

$$P(A,B \mid C)=P(A \mid C)P(B \mid A, C)=P(B \mid C)P(A \mid B,C)$$

where the term $$P(A \mid B)$$ can be interpreted as the probability of the truth of proposition $$A$$, given (conditional on) the truth of the information represented by proposition $$B$$. Bayes Theorem then follows from the product rule by rearranging the middle and right equations,

$$P(A \mid B,C)= \frac{P(A \mid C) P(B \mid A,C)}{P(B \mid C)}$$

### (1.3) : Usual Form of Bayes' Theorem

$$P(H_i  ┤|D, I)=(P(H_i│I)P(D|H_i,I))/(P(D|I))$$
- $$H_i$$ : proposition asserting the truth of a hypothesis.
- $$I$$ : proposition representing our prior information.
- $$D$$ : proposition representing data.
- $$P(D│H_i,I)$$ : probability of obtaining the data, $$D$$, if $$H_i$$  and $$I$$ are true. Also called the likelihood.
- $$P(H_i |I)$$ : prior probability of the hypothesis.
- $$P(H_i |D,I)$$ : posterior probability of $$H_i$$.
- $$P(D│I)= Σ_i  P(H_i│I)p(D│H_i,I)$$ : normalization factor.
